{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8b424c-4d73-496e-ac0b-5970e40cefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems\n",
    "from langchain.prompts import PromptTemplate\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b4e845-0859-4d7f-b319-4116def9efe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa92951a44e04ad99730966ca86d2eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b280fcf52c14176829028aaf8dbd4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f1be9d7c947678f52fdc2cbcf3a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bca3d2c5f4477984006ea80908566b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcb5beb081d4256934d5af8a48b8dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = 'http://0.0.0.0:8000/v1/completions'\n",
    "with open('prompt_com.txt', 'r') as file:\n",
    "    template_content = file.read()\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_text\"],\n",
    "    template=template_content)\n",
    "\n",
    "def enhance_prompt(item, max_cases=2, seed=42):\n",
    "\n",
    "    random.seed(seed)\n",
    "    prompt = item[\"prompt\"]\n",
    "    test_code = item.get(\"test\", \"\")\n",
    "    function_name = item.get('entry_point', '')\n",
    "\n",
    "    asserts = re.findall(r\"assert (candidate\\(.*?\\)\\s*==\\s*(?:True|False))\", test_code)\n",
    "\n",
    "    if not asserts:\n",
    "        return prompt\n",
    "\n",
    "    true_cases = [a for a in asserts if a.endswith(\"True\")]\n",
    "    false_cases = [a for a in asserts if a.endswith(\"False\")]\n",
    "\n",
    "    selected = []\n",
    "    if true_cases:\n",
    "        selected.append(random.choice(true_cases))\n",
    "    if false_cases:\n",
    "        selected.append(random.choice(false_cases))\n",
    "\n",
    "    if len(selected) < max_cases:\n",
    "        extra = list(set(asserts) - set(selected))\n",
    "        if extra:\n",
    "            selected.extend(random.sample(extra, min(max_cases - len(selected), len(extra))))\n",
    "\n",
    "    selected = [v.replace('candidate', function_name) for v in selected]\n",
    "    comment_cases = \"    # some more testing cases:\\n\" + \"\\n\".join([f\"    # {case}\" for case in selected])\n",
    "\n",
    "    enhanced_prompt = prompt.rstrip() + \"\\n\" + comment_cases\n",
    "    return enhanced_prompt\n",
    "\n",
    "def process_output(response: str, task_description: str) -> str:\n",
    "    \"\"\"\n",
    "    remove the prompt from a generation result. HumanEval only requires the generated part for it's evaluation.\n",
    "    no longer used\n",
    "\n",
    "    :param response: the generation result of an LLM\n",
    "    :param task_description: the prompt and function head string\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    response = response.replace(task_description, \"\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_single(problem_text: str, temp=.2, n=1):\n",
    "    \"\"\"\n",
    "    The main inference function\n",
    "\n",
    "    :param problem_text: the code problem without any preprocessing\n",
    "    :param temp: generation temperature\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assembled_prompt = prompt_template.format(problem_text=problem_text)\n",
    "\n",
    "    if temp == 0:\n",
    "        n = 1\n",
    "    \n",
    "    data = {\n",
    "        \"prompt\": assembled_prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": temp,\n",
    "        \"stop\": [\"\\n\\n\", \"\\nclass \", \"\\ndef \", \"<|im_end|>\", \"</s>\"],\n",
    "        \"top_k\": 20, \n",
    "        \"top_p\": 0.85,\n",
    "        \"n\": n,\n",
    "        # \"repetition_penalty\": 1.02,\n",
    "        \"logits_processor\": [\n",
    "            {\"type\": \"ban_tokens\", \"banned_ids\": [6385, 750, 1112]}]\n",
    "    }\n",
    "    # print(assembled_prompt)\n",
    "    response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        generated = result[\"choices\"]\n",
    "        \n",
    "        return [v['text'] for v in generated]\n",
    "        \n",
    "    else:\n",
    "        return ['pass']\n",
    "\n",
    "\n",
    "problems = read_problems('human-eval/data/HumanEval.jsonl.gz')\n",
    "temps = [0, .05, .1, .2, .4]\n",
    "ns = 10\n",
    "for t in temps:\n",
    "    samples = []\n",
    "    for task_id in tqdm(problems):\n",
    "        function_head = enhance_prompt(problems[task_id])\n",
    "        generated = generate_single(function_head, t, ns)\n",
    "        samples.extend([dict(task_id=task_id, completion=generated[j]) for j in range(len(generated))])\n",
    "    write_jsonl(f\"nsamples_{t}_{ns}.jsonl\", samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4d071-a267-4adf-8982-41bffd4e9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
