{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982d562a-7d6e-40da-9d58-ff8d58db332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'coding-tasks'...\n",
      "remote: Enumerating objects: 30, done.\u001B[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001B[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001B[K\n",
      "remote: Total 30 (delta 6), reused 7 (delta 4), pack-reused 18 (from 1)\u001B[K\n",
      "Unpacking objects: 100% (30/30), 47.54 KiB | 4.75 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maoquan-ms/coding-tasks.git data/coding-tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b424c-4d73-496e-ac0b-5970e40cefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems\n",
    "from langchain.prompts import PromptTemplate\n",
    "import requests\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "we are not using complex prompts or agent functionalities, so let's have a direct call to the vllm service"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ce1c26-8f3f-4843-9baf-f9e039f99a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://0.0.0.0:8000/generate'\n",
    "with open('../data/prompt_coder.txt', 'r') as file:\n",
    "    template_content = file.read()\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_text\"],\n",
    "    template=template_content)\n",
    "\n",
    "def process_output(response: str, task_description: str) -> str:\n",
    "    \"\"\"\n",
    "    remove the prompt from a generation result. HumanEval only requires the generated part for it's evaluation.\n",
    "\n",
    "    :param response: the generation result of an LLM\n",
    "    :param task_description: the prompt and function head string\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    response = response.replace(task_description, \"\").strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_single(problem_text: str, temp=.2):\n",
    "    \"\"\"\n",
    "    The main inference function\n",
    "\n",
    "    :param problem_text: the code problem without any preprocessing\n",
    "    :param temp: generation temperature\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assembled_prompt = prompt_template.format(problem_text=problem_text)\n",
    "    \n",
    "    data = {\n",
    "        \"prompt\": assembled_prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": temp,\n",
    "        \"stop\": [\"\\n\\n\", \"\\ndef\", \"\\nclass \"],\n",
    "        \"top_k\": 40, \n",
    "        \"top_p\": 0.85\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        generated_code = result[\"text\"]\n",
    "\n",
    "        return [process_output(v, assembled_prompt) for v in generated_code]\n",
    "    else:\n",
    "        return ['pass']\n",
    "\n",
    "def generate_multiple(problem_text: str, n=5, temp=.2):\n",
    "    rslt = []\n",
    "    for _ in range(n):\n",
    "        rslt.extend(generate_single(problem_text, temp))\n",
    "    return rslt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inference with different generation temperatures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf17e7cf-b5d7-4f98-84db-ec29480ce26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4049b92daca1467e8fb51d6a37727841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133c9ea147f44aa3924798f162effaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea3689d3c7243ce8f6bbf85e45bc9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "problems = read_problems('../data/coding-tasks/datasets/humaneval/HumanEval.jsonl')\n",
    "temps = [.2, .4, .6, .8]\n",
    "ns = 10\n",
    "for t in temps:\n",
    "    samples = []\n",
    "    for task_id in tqdm(problems):\n",
    "        for _ in range(ns):\n",
    "            samples.append(dict(task_id=task_id, completion=generate_single(problems[task_id][\"prompt\"], t)[0]))\n",
    "\n",
    "\n",
    "    write_jsonl(f\"samples_{t}_{ns}.jsonl\", samples)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
